{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\ntqdm.pandas(desc=\"progress-bar\")\nfrom gensim.models import Doc2Vec\nfrom sklearn import utils\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.sequence import pad_sequences\nimport gensim\nfrom sklearn.linear_model import LogisticRegression\nfrom gensim.models.doc2vec import TaggedDocument\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df = pd.read_csv('../input/reverse-balance-8700/rb 8700.csv',delimiter=',',encoding='latin-1')\ndf = df[['Tweet','Kelas']]\ndf = df[pd.notnull(df['Tweet'])]\ndf.rename(columns = {'Tweet':'Tweet'}, inplace = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":false},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df.index = range(8700)\ndf['Tweet'].apply(lambda x: len(x.split(' '))).sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"cnt_pro = df['Kelas'].value_counts()\nplt.figure(figsize=(12,4))\nsns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Kelas', fontsize=12)\nplt.xticks(rotation=90)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"def print_complaint(index):\n    example = df[df.index == index][['Tweet', 'Kelas']].values[0]\n    if len(example) > 0:\n        print(example[0])\n        print('Tweet:', example[1])\nprint_complaint(12)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print_complaint(0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Text Preprocessing Below we define a function to convert text to lower-case and strip punctuation/symbols from words and so on."},{"metadata":{"trusted":false},"cell_type":"code","source":"from bs4 import BeautifulSoup\ndef cleanText(text):\n    text = BeautifulSoup(text, \"lxml\").text\n    text = re.sub(r'\\|\\|\\|', r' ', text) \n    text = re.sub(r'http\\S+', r'<URL>', text)\n    text = text.lower()\n    text = text.replace('x', '')\n    return text\ndf['Tweet'] = df['Tweet'].apply(cleanText)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"df['Tweet'] = df['Tweet'].apply(cleanText)\ntrain, test = train_test_split(df, test_size=0.000001 , random_state=42)\nimport nltk\nfrom nltk.corpus import stopwords\ndef tokenize_text(text):\n    tokens = []\n    for sent in nltk.sent_tokenize(text):\n        for word in nltk.word_tokenize(sent):\n            #if len(word) < 0:\n            if len(word) <= 0:\n                continue\n            tokens.append(word.lower())\n    return tokens\ntrain_tagged = train.apply(\n    lambda r: TaggedDocument(words=tokenize_text(r['Tweet']), tags=[r.Kelas]), axis=1)\ntest_tagged = test.apply(\n    lambda r: TaggedDocument(words=tokenize_text(r['Tweet']), tags=[r.Kelas]), axis=1)\n\n# The maximum number of words to be used. (most frequent)\nmax_fatures = 500000\n\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 50\n\n#tokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer = Tokenizer(num_words=max_fatures, split=' ', filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(df['Tweet'].values)\nX = tokenizer.texts_to_sequences(df['Tweet'].values)\nX = pad_sequences(X)\nprint('Found %s unique tokens.' % len(X))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X = tokenizer.texts_to_sequences(df['Tweet'].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(X[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(X[1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(X[2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"train_tagged","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#train_tagged.values[2173]\ntrain_tagged.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"kata=train_tagged.values\nprint(kata[2170:2190])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#train_tagged[2173]\ntrain_tagged","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"test_tagged.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model DM = 1\n\nThis work use DM=1 (it preserve word order)\n\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"d2v_model = Doc2Vec(dm=1, dm_mean=1, size=20, window=8, min_count=1, workers=1, alpha=0.065, min_alpha=0.065)\nd2v_model.build_vocab([x for x in tqdm(train_tagged.values)])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"%%time\nfor epoch in range(30):\n    d2v_model.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n    d2v_model.alpha -= 0.002\n    d2v_model.min_alpha = d2v_model.alpha","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"print(d2v_model)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"len(d2v_model.wv.vocab)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##  Save the vectors in a new matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"#print(word_model) \n# save the vectors in a new matrix\n#embedding_matrix = np.zeros((len(model.wv.vocab)+ 1, 100))\n#embedding_matrix = np.ones((len(model.wv.vocab)+ 1, 100))\n#nonzero ndarray\nembedding_matrix = np.zeros((len(d2v_model.wv.vocab)+ 1, 20))\n\nfor i, vec in enumerate(d2v_model.docvecs.vectors_docs):\n    while i in vec <= 1000:\n    #print(i)\n    #print(model.docvecs)\n          embedding_matrix[i]=vec\n    #print(vec)\n    #print(vec[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Measuring distance between two vectors (related to cosine similarity)"},{"metadata":{"trusted":false},"cell_type":"code","source":"d2v_model.wv.most_similar(positive=['optimis','sabar'], topn=50)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d2v_model.wv.doesnt_match(['selalu', 'optimis','dan','sabar','pak','jokowi'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d2v_model.wv.doesnt_match(['selamat', 'ulang', 'tahun', 'bagi', 'negara', 'khusus', 'islam', ',', 'indonesia', '.', 'semoga', 'ke', 'depannya', 'bisa', 'semakin', 'menyudutkan', 'rakyat', 'non-muslim'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"d2v_model.wv.most_similar(positive=['jokowi'], topn=35)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot Similarity word in Doc2vec"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef tsne_plot(model):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in d2v_model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=500, random_state=42)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(16, 16)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tsne_plot(d2v_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KeyedVectors : Store and query word vectors\n\nYou can perform various syntactic/semantic NLP word tasks with the trained vectors. Some of them are already built-in"},{"metadata":{"trusted":false},"cell_type":"code","source":"d2v_model.wv.vocab","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model LSTM\n\n"},{"metadata":{"trusted":false},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Embedding\n\n\n# init layer\nmodel = Sequential()\n\n# emmbed word vectors\nmodel.add(Embedding(len(d2v_model.wv.vocab)+1,20,input_length=X.shape[1],weights=[embedding_matrix],trainable=True))\n\n# learn the correlations\ndef split_input(sequence):\n     return sequence[:-1], tf.reshape(sequence[1:], (-1,1))\nmodel.add(LSTM(50,return_sequences=False))\nmodel.add(Dense(2,activation=\"softmax\"))\n\n# output model skeleton\nmodel.summary()\nmodel.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Y = pd.get_dummies(df['Kelas']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"batch_size = 32\nmodel.fit(X_train, Y_train, epochs =20, batch_size=batch_size, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Validation"},{"metadata":{"trusted":false},"cell_type":"code","source":"validation_size = 860\n\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore,acc = model.evaluate(X_test, Y_test, verbose = 1, batch_size = batch_size)\n#score,acc = model.evaluate(X_test, Y_test)\n\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  Evaluate the model (train split 90:10)\nThe test from 90:10 split data should give as only the quality of the model."},{"metadata":{"trusted":false},"cell_type":"code","source":"# evaluate the model\n_, train_acc = model.evaluate(X_train, Y_train, verbose=2)\n_, test_acc = model.evaluate(X_test, Y_test, verbose=2)\nprint('Train: %.3f, Test: %.4f' % (train_acc, test_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# predict probabilities for test set (validation)\nyhat_probs = model.predict(X_test, verbose=0)\n#print(yhat_probs)\n# predict crisp classes for test set(validation)\nyhat_classes = model.predict_classes(X_test, verbose=0)\n#print(yhat_classes)\n# reduce to 1d array\nyhat_probs = yhat_probs[:, 0]\n#yhat_classes = yhat_classes[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"import numpy as np\nrounded_labels=np.argmax(Y_test, axis=1)\nrounded_labels","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### The confusion matrix"},{"metadata":{"trusted":false},"cell_type":"code","source":"# The confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nlstm_val = confusion_matrix(rounded_labels, yhat_classes)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(lstm_val, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"YlGnBu\")\nplt.title('LSTM Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Real Test \n\nWe compared a more realistic test accuracy of the model using new and different data set aside from the data to build the model."},{"metadata":{"trusted":false},"cell_type":"code","source":"new_test = [\"#4niesbebalsokhebat tanggul. jebol perbaiki tuk anda menunggu patungan harus sampai ya? hebat @aniesbaswedan anda warga \"] #reverse tweet \nseq = tokenizer.texts_to_sequences(new_test)\n\npadded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n\npred = model.predict(padded)\n\n# 0:Non. 1: Sarcasm\nlabels = ['0','1']\n\n#print vektor asli\nprint(seq)\n\n#print yang di pad\nprint(padded)\n\n#print prediction\nprint(pred, labels[np.argmax(pred)])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test 120 Tweets (in which 56 are sarcastic and the other are not). you can find it on test folder : https://github.com/skhotijah/using-lstm-for-context-based-approach-of-sarcasm-detection-in-twitter/tree/main/test"},{"metadata":{"trusted":false},"cell_type":"code","source":"model.predict_classes(\n    pad_sequences(\n         tokenizer.texts_to_sequences(\n             [ \". dicintainya pantas yang siapa tau lebih akan terluka pernah yang cewek tetapi . seseorang mencintai bagaimana tau pasti pintar yang cewek \",\n                \"hehe flashmopnya dikit ikutan juga tadi gabung untuk juga serta ikut yang warga banyak rame kegiatanya #rismaselamanya pendukung simpatisan gerakan ada pagi tadi bungkul taman cfd \",\n                 \"#4niesbebalsokhebat alhamdulilah syukur @wagimandeep \",\n                \"#4niesbebalsokhebat .. siapa klaim yang siapa kerja yang ... ya hebat memang @aniesbaswedan bapak \",\n                \"#4niesbebalsokhebat ..??????????????? hebat..aduuuh merasa susudah doang ngomong bisa hanya ya kerjakan di bisa tidak tapi ceritakan anies bisa semua lewat ilmunya lain orang .... hebat memang anies @73hermantjoe \"]),\n         maxlen=MAX_SEQUENCE_LENGTH))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"model.save('mycoolmodel.h5')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}