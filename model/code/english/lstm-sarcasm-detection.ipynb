{"cells":[{"metadata":{"_cell_guid":"e6985cac-11d9-458a-b3f5-c1b2fddb236b","_uuid":"1fa18345-8e07-4bcb-8134-b7edd1b06443","trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\ntqdm.pandas(desc=\"progress-bar\")\nfrom gensim.models import Doc2Vec\nfrom sklearn import utils\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.sequence import pad_sequences\nimport gensim\nfrom sklearn.linear_model import LogisticRegression\nfrom gensim.models.doc2vec import TaggedDocument\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"61b776e4-c278-4630-8799-a8f70004585f","_uuid":"a4cb5aed-8de2-4362-b183-ba5ea0d1b1f9","trusted":false},"cell_type":"code","source":"df = pd.read_csv('../input/data-train/sarcasm.csv',delimiter=',',encoding='latin-1')\ndf = df[['Reverse','label']]\ndf = df[pd.notnull(df['Reverse'])]\ndf.rename(columns = {'Reverse':'Reverse'}, inplace = True)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"514a445c-9800-48a0-a4a2-6065decfddbe","_uuid":"3de56c02-20e6-401b-b85c-2f37715ab1a5","trusted":false},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1133bb2d-bbd8-44c6-960f-4bc26dbc840c","_uuid":"6d9eae35-b8b8-4808-a7b8-baa2dababd7e","trusted":false},"cell_type":"code","source":"df.index = range(1800)\ndf['Reverse'].apply(lambda x: len(x.split(' '))).sum()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"50ea3af5-3597-4016-b8bd-fa3bfded3d21","_uuid":"d614f909-1b10-42cd-9d9c-d94ea68492c2","trusted":false},"cell_type":"code","source":"cnt_pro = df['label'].value_counts()\nplt.figure(figsize=(12,4))\nsns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)\nplt.ylabel('Number of Tweet', fontsize=12)\nplt.xlabel('label', fontsize=12)\nplt.xticks(rotation=90)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"cdc27d9e-b511-4d5f-b189-d71ed96656b1","_uuid":"c78aa971-96c9-47c2-b15e-020d77b43366","trusted":false},"cell_type":"code","source":"def print_complaint(index):\n    example = df[df.index == index][['Reverse', 'label']].values[0]\n    if len(example) > 0:\n        print(example[0])\n        print('Reverse:', example[1])\nprint_complaint(12)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"812fb6d5-31f2-4819-8735-832df6a960a7","_uuid":"b3a556e0-bee1-4bb6-88c6-1ab886489ede","trusted":false},"cell_type":"code","source":"print_complaint(0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"eb072c16-1e46-4789-956e-39915088f587","_uuid":"8627d942-f5f6-4b6e-ab68-bdd7535be315"},"cell_type":"markdown","source":"Text Preprocessing Below we define a function to convert text to lower-case and strip punctuation/symbols from words and so on."},{"metadata":{"_cell_guid":"9e24d5d2-4373-434e-b6a1-3b45c134800c","_uuid":"e9200ad3-6f59-4379-ae49-efb0639a42c4","trusted":false},"cell_type":"code","source":"from bs4 import BeautifulSoup\ndef cleanText(text):\n    text = BeautifulSoup(text, \"lxml\").text\n    text = re.sub(r'\\|\\|\\|', r' ', text) \n    text = re.sub(r'http\\S+', r'<URL>', text)\n    text = text.lower()\n    text = text.replace('x', '')\n    return text\ndf['Reverse'] = df['Reverse'].apply(cleanText)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a26d4d75-3772-46bb-8399-2c447e6fe57f","_uuid":"7261cfe9-107c-4e6e-8a5b-e9c3e8baadd4","trusted":false},"cell_type":"code","source":"df['Reverse'] = df['Reverse'].apply(cleanText)\ntrain, test = train_test_split(df, test_size=0.000001 , random_state=42)\nimport nltk\nfrom nltk.corpus import stopwords\ndef tokenize_text(text):\n    tokens = []\n    for sent in nltk.sent_tokenize(text):\n        for word in nltk.word_tokenize(sent):\n            #if len(word) < 0:\n            if len(word) <= 0:\n                continue\n            tokens.append(word.lower())\n    return tokens\ntrain_tagged = train.apply(\n    lambda r: TaggedDocument(words=tokenize_text(r['Reverse']), tags=[r.label]), axis=1)\ntest_tagged = test.apply(\n    lambda r: TaggedDocument(words=tokenize_text(r['Reverse']), tags=[r.label]), axis=1)\n\n# The maximum number of words to be used. (most frequent)\nmax_fatures = 1358\n\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 50\n\n#tokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer = Tokenizer(num_words=max_fatures, split=' ', filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(df['Reverse'].values)\nX = tokenizer.texts_to_sequences(df['Reverse'].values)\nX = pad_sequences(X)\nprint('Found %s unique tokens.' % len(X))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"175b9e8a-0865-4ec5-92b6-c5602a007863","_uuid":"ed40482f-ad2f-45dd-8cc9-96c288f39631","trusted":false},"cell_type":"code","source":"X = tokenizer.texts_to_sequences(df['Reverse'].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3a4dad2d-5ca6-4711-8b79-ba1a8a6adc2b","_uuid":"c672c22e-3b6d-43b9-b94a-73598082162a","trusted":false},"cell_type":"code","source":"train_tagged","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"fdeb127d-628b-4f01-961b-ac10f3b2e4f2","_uuid":"152acd7e-3205-46f0-8ae0-d7bc2d7e5bed","trusted":false},"cell_type":"code","source":"#train_tagged.values[2173]\ntrain_tagged.values","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"09789236-6209-4dd6-8a5d-9c9f47ecaf8d","_uuid":"bd82546e-6083-44be-91c3-c2b58be25cad","trusted":false},"cell_type":"code","source":"kata=train_tagged.values\nprint(kata[0:21])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5eef258a-b9cd-46fa-b217-dc5f5bcae759","_uuid":"2f264c90-89d7-4acb-8889-b69664b9a87c","trusted":false},"cell_type":"code","source":"#train_tagged[2173]\ntrain_tagged","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"384d70e4-cc53-480e-ab0b-5d2fc7496f21","_uuid":"3c1d0066-cf1d-4bd5-b84e-a624ce4fff5e","trusted":false},"cell_type":"code","source":"test_tagged.values","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"55a1232c-9266-4ccc-a2af-435bf876b748","_uuid":"858d5891-c58f-4929-af3c-dbd369b8a4b6"},"cell_type":"markdown","source":"This work use DM=1 (it preserve word order)"},{"metadata":{"_cell_guid":"61b3a6b8-d62c-4966-87a3-04a83e353408","_uuid":"724a1ac4-bce8-4c75-b2e4-8be7fcd86a8b","trusted":false},"cell_type":"code","source":"d2v_model = Doc2Vec(dm=1, dm_mean=1, size=20, window=8, min_count=1, workers=1, alpha=0.065, min_alpha=0.065)\nd2v_model.build_vocab([x for x in tqdm(train_tagged.values)])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e7bb414c-aa39-438e-93f2-33d73ade5e0f","_uuid":"696eb57f-83f8-457c-9ed8-0e143403edf8","trusted":false},"cell_type":"code","source":"%%time\nfor epoch in range(30):\n    d2v_model.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n    d2v_model.alpha -= 0.002\n    d2v_model.min_alpha = d2v_model.alpha","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"de692fe8-9c7b-40a9-9ae0-88d9ea0efd15","_uuid":"60ac6233-5f4b-4fca-ba02-28889dead701","trusted":false},"cell_type":"code","source":"print(d2v_model)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ddc7bbca-22eb-4a7b-884c-6942b64897b1","_uuid":"fb3239de-c11b-47a6-be66-05999fc34e08","trusted":false},"cell_type":"code","source":"len(d2v_model.wv.vocab)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d39e19f4-0c76-4e4a-a8bb-43beea7e7d3d","_uuid":"3c1c3c23-6ddd-47dc-94dc-48702156fc84","trusted":false},"cell_type":"code","source":"#print(word_model) \n# save the vectors in a new matrix\n#embedding_matrix = np.zeros((len(model.wv.vocab)+ 1, 100))\n#embedding_matrix = np.ones((len(model.wv.vocab)+ 1, 100))\n#nonzero ndarray\nembedding_matrix = np.zeros((len(d2v_model.wv.vocab)+ 1, 20))\n\nfor i, vec in enumerate(d2v_model.docvecs.vectors_docs):\n    while i in vec <= 1000:\n    #print(i)\n    #print(model.docvecs)\n          embedding_matrix[i]=vec\n    #print(vec)\n    #print(vec[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Measuring distance between two vectors (related to cosine similarity)"},{"metadata":{"_cell_guid":"e5a56353-efb1-48a7-acac-a3e4825b1ab5","_uuid":"0fa5fc41-ee51-478a-95dc-7eeaebb983e6","trusted":false},"cell_type":"code","source":"d2v_model.wv.most_similar(positive=['love','not'], topn=50)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a9d7e323-0679-46f0-a953-36afbdc8b8b8","_uuid":"01d0cb55-d46d-41c1-8f0b-c900ca09b968","trusted":false},"cell_type":"code","source":"d2v_model.wv.doesnt_match(['i', 'love', 'feeling', 'like', 'a', 'second', 'choice'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"920a6fe9-34a4-4509-856b-e0a8178849de","_uuid":"9c1e22ef-7558-4c32-a0e4-dabeb7a2d84c","trusted":false},"cell_type":"code","source":"d2v_model.wv.most_similar(positive=['glad'], topn=35)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Plot Similarity word in Doc2vec"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef tsne_plot(model):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in d2v_model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=500, random_state=42)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(16, 16)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"tsne_plot(d2v_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## KeyedVectors : Store and query word vectors\n\nYou can perform various syntactic/semantic NLP word tasks with the trained vectors. Some of them are already built-in"},{"metadata":{"_cell_guid":"ff8938c6-10cb-43f4-b517-f87a450f96f4","_uuid":"eecf0966-a349-4cfb-8b4a-ad1651935064","trusted":false},"cell_type":"code","source":"d2v_model.wv.vocab","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"27bbb9e4-b6d3-432c-9c54-00c08b6dd0f4","_uuid":"dc77274b-176e-4a2e-8f92-6bf9fc1551f9"},"cell_type":"markdown","source":"# Model LSTM"},{"metadata":{"_cell_guid":"14e4bec3-cce3-480c-8168-7c023541f224","_uuid":"f32a723c-212d-4730-941e-fc927ba86787","trusted":false},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Embedding\n\n\n# init layer\nmodel = Sequential()\n\n# emmbed word vectors\nmodel.add(Embedding(len(d2v_model.wv.vocab)+1,20,input_length=X.shape[1],weights=[embedding_matrix],trainable=True))\n\n# learn the correlations\ndef split_input(sequence):\n     return sequence[:-1], tf.reshape(sequence[1:], (-1,1))\nmodel.add(LSTM(50,return_sequences=False))\nmodel.add(Dense(2,activation=\"softmax\"))\n\n# output model skeleton\nmodel.summary()\nmodel.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"9adaafa5-be4c-49f9-ae02-2e9fbae97e30","_uuid":"caaa9a00-3da9-4655-8dac-d0c502f614b4","trusted":false},"cell_type":"code","source":"Y = pd.get_dummies(df['label']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"426a0ebc-cd8c-4aeb-93ca-71375b0c537e","_uuid":"47ee5e8e-50d6-41d9-812d-e85c61fa7c75","trusted":false},"cell_type":"code","source":"batch_size = 32\nmodel.fit(X_train, Y_train, epochs =1000, batch_size=batch_size, verbose = 2)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7ad7ea3b-156e-4d34-8660-6fcbc0a02ecb","_uuid":"c9469430-843b-46d5-8549-1ec51faa3a3a"},"cell_type":"markdown","source":"## validation"},{"metadata":{"_cell_guid":"c9e1c15b-1c6d-42ed-8ff5-ff4a0571c0c3","_uuid":"d246f9ac-5338-4939-adf6-6c9407c544b5","trusted":false},"cell_type":"code","source":"validation_size = 150\n\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore,acc = model.evaluate(X_test, Y_test, verbose = 1, batch_size = batch_size)\n#score,acc = model.evaluate(X_test, Y_test)\n\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1816b906-4de2-4991-acac-ca1c43826710","_uuid":"a677031f-a830-4e0e-b018-1f9681fb08ac"},"cell_type":"markdown","source":"## Evaluate the model (train split 90:10)\n\nThe test from 90:10 split data should give as only the quality of the model."},{"metadata":{"_cell_guid":"302d0fa1-4978-4e0a-bfb4-48dcf739adb4","_uuid":"e7671c1e-eeb6-44e8-abc2-0fd99998e2a0","trusted":false},"cell_type":"code","source":"# evaluate the model\n_, train_acc = model.evaluate(X_train, Y_train, verbose=2)\n_, test_acc = model.evaluate(X_test, Y_test, verbose=2)\nprint('Train: %.3f, Test: %.4f' % (train_acc, test_acc))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e0208a17-3dde-4fa0-8611-cc3b96f58343","_uuid":"cbc9c4e0-d70e-499d-bb7c-886c4aee2eaf","trusted":false},"cell_type":"code","source":"# predict probabilities for test set (validation)\nyhat_probs = model.predict(X_test, verbose=0)\n#print(yhat_probs)\n# predict crisp classes for test set(validation)\nyhat_classes = model.predict_classes(X_test, verbose=0)\n#print(yhat_classes)\n# reduce to 1d array\nyhat_probs = yhat_probs[:, 0]\n#yhat_classes = yhat_classes[:, 1]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3f50c194-7842-43ac-9470-a8cf4e9e2b77","_uuid":"82120b39-f77c-47d9-89d2-8b4bd2686f8b","trusted":false},"cell_type":"code","source":"import numpy as np\nrounded_labels=np.argmax(Y_test, axis=1)\nrounded_labels","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"adc5efd1-f755-47cb-aee1-aaacec55684f","_uuid":"1885983e-4d4e-412b-acfa-1df46b508d7e"},"cell_type":"markdown","source":"## The confusion matrix"},{"metadata":{"_cell_guid":"6fd5842a-a938-49d1-bbe9-154260f039cd","_uuid":"b0955879-9925-4f91-9c90-6cc501aeeb2d","trusted":false},"cell_type":"code","source":"# The confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nlstm_val = confusion_matrix(rounded_labels, yhat_classes)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(lstm_val, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"YlGnBu\")\nplt.title('LSTM Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e3046042-036f-454f-941c-e30cae07743c","_uuid":"bdb7340a-08d6-46df-a501-b83527e2c092"},"cell_type":"markdown","source":"## Real Test \n\nWe compared a more realistic test accuracy of the model using new and different data set aside from the data to build the model."},{"metadata":{},"cell_type":"markdown","source":"example"},{"metadata":{"_cell_guid":"fcd23066-7ec6-4c49-95e6-33b96eccb0d1","_uuid":"99f36c47-ec75-460e-b428-3cb5b734929b","trusted":false},"cell_type":"code","source":"#Original Love it when thunder wakes me up at 830\n\nnew_test = ['830 at up me wakes thunder when it Love'] #reverse tweet \nseq = tokenizer.texts_to_sequences(new_test)\n\npadded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n\npred = model.predict(padded)\n\n# 0:Non. 1: Sarcasm\nlabels = ['0','1']\n\n#print vektor asli\nprint(seq)\n\n#print yang di pad\nprint(padded)\n\n#print prediction\nprint(pred, labels[np.argmax(pred)])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a01bb420-f595-4ce3-99f8-5f0fd41513c8","_uuid":"564b6781-cba1-4ddb-9760-26b6b4d44b29","trusted":false},"cell_type":"code","source":"#Original I just love missing the bus! ?\n\nnew_test = ['? bus! the missing love just I'] #reverse tweet \nseq = tokenizer.texts_to_sequences(new_test)\n\npadded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n\npred = model.predict(padded)\n\n# 0:Non. 1: Sarcasm\nlabels = ['0','1']\n\n#print sequence \nprint(seq)\n\n#print sequence that was padded\nprint(padded)\n\n#print prediction\nprint(pred, labels[np.argmax(pred)])","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f3c99031-3bdc-4c52-945d-29879415c51f","_uuid":"044d3223-81ec-47f8-84b9-870b53226db9"},"cell_type":"markdown","source":"Test 200 Tweets (in which 100 are sarcastic and the other are not).\nyou can find it on test folder : https://github.com/skhotijah/using-lstm-for-context-based-approach-of-sarcasm-detection-in-twitter/tree/main/test"},{"metadata":{"_cell_guid":"7893f60f-a2e8-4da4-883d-5f1d661237ca","_uuid":"2cac8a2f-e708-4b1c-b905-5e39ab36196e","trusted":false},"cell_type":"code","source":"model.predict_classes(\n    pad_sequences(\n         tokenizer.texts_to_sequences(\n             [\"URL out! it check and over Hop giveaway! this in entries low Very \",\n                   \":') week my off topped totally This \",\n                  \"reply a getting not and USER tweeting enjoy I \",\n                   \"#education paid. getting without working is teaching of joys greatest the of One \",                              \n                   \"purpose on dumb act girls when cute so it's think I \",\n                   \":) much so it like I up keeps this hope I answer. an get &don't question important an ask I when it like really I \",\n                   \"#notimpressed convicted. really you're see can Wow..I \",\n                   \"#GrowUp #Annoyed funny. Thats , Hah\",\n                   \"830 at up me wakes thunder when it Love \",\n                   \"you have to glad are, you friends good proper today, me ditching for thanks\",\n                   \"do! to something find to Need . happen. never that plans love I \"]),\n         maxlen=MAX_SEQUENCE_LENGTH))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6bec83bf-cce6-42c9-acfa-c37499dd46a5","_uuid":"052ef181-5d1e-47b4-a382-2d39b34dabe2","trusted":false},"cell_type":"code","source":"model.save('model.h5')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}